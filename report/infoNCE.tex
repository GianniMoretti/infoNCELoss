% !TEX program = pdflatex
\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{hyperref}

\title{InfoNCE Loss Derivation}
\author{}
\date{}

\begin{document}

\maketitle

\section*{Introduction}
This document provides a detailed derivation of the InfoNCE loss gradient with respect to the feature vectors $Z$. The InfoNCE loss is used in contrastive \emph{self-supervised learning} methods to bring "positive" example pairs closer together and push "negative" examples apart in the representation space.

\section{Loss definition}
Let $N=2B$ be the total number of feature vectors and $Z=\{Z_1, Z_2, \dots, Z_N\}$ with $Z_i\in\mathbb{R}^d$ and $\|Z_i\|_2=1$. We define:
\begin{align*}
L_{ij} &= \frac{1}{\tau} \, Z_i \cdot Z_j, \\
P_{ij} &= \frac{\exp(L_{ij})}{\sum_{k=1}^{N} \exp(L_{ik})}, \\
\mathcal{L} &= -\frac{1}{N} \sum_{i=1}^{N} \log P_{i, p(i)},
\end{align*}
where $p(i)=(i+B)\bmod N$ identifies the index of the positive sample for each row $i$.

\section{Derivative with respect to logits}
For each row $i$ we consider
\[
\ell_i = -\log P_{i,p(i)}
\]
The derivative of $\ell_i$ with respect to $L_{ij}$, applying cross-entropy on softmax, is:
\[
\frac{\partial \ell_i}{\partial L_{ij}} = P_{ij} - \begin{cases}1 & j = p(i), \\ 0 & j \neq p(i).\end{cases}
\]
Since $\mathcal{L}=\frac{1}{N}\sum_i \ell_i$, then
\[
\frac{\partial \mathcal{L}}{\partial L_{ij}} = \frac{1}{N}\bigl(P_{ij} - \mathbb{1}_{j=p(i)}\bigr).
\]

\section{Derivative of logits with respect to Z}
Remembering $L_{ij}=\tfrac{1}{\tau} Z_i \cdot Z_j$, for each vector component $Z_k$ we have:
\[
\frac{\partial L_{ij}}{\partial Z_k} = \frac{1}{\tau}(\delta_{ik} Z_j + \delta_{jk} Z_i),
\]
where $\delta_{ik}$ is the Kronecker delta.

\section{Final derivation chain}
Applying the chain rule:
\[
\frac{\partial \mathcal{L}}{\partial Z_k} = \sum_{i,j} \frac{\partial \mathcal{L}}{\partial L_{ij}} \, \frac{\partial L_{ij}}{\partial Z_k}.
\]
Separating the contributions and defining matrix $G$ with
\[
G_{ij} = P_{ij} - \mathbb{1}_{j=p(i)},
\]
we obtain:
\[
\frac{\partial \mathcal{L}}{\partial Z_k} = \frac{1}{N \tau} \Bigl(\sum_{j} G_{kj} Z_j + \sum_{i} G_{ik} Z_i\Bigr).
\]
In compact form:
\[
\nabla_Z \mathcal{L} = \frac{1}{N \tau} (G + G^T) Z.
\]

\section*{Conclusions}
The derivation shows how the InfoNCE loss gradient is obtained from the combination of softmax probabilities corrected by positive markers, multiplied by the feature matrix. The symmetric term $(G+G^T)$ ensures that each feature pair contributes reciprocally to pushing toward positives and away from negatives.

\end{document}
